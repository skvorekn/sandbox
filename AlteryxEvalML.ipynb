{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial based on https://github.com/alteryx/evalml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO methodology ideas:\n",
    "* algorithmic bias\n",
    "* calibration objective\n",
    "* sensitivity at low alert rates objective\n",
    "\n",
    "### Workflow\n",
    "* semantic commits\n",
    "* git flow\n",
    "\n",
    "### Documentation\n",
    "* add problem_type argument to documentation\n",
    "* update woodwork documentation without the dreaded value slice error: https://woodwork.alteryx.com/en/stable/guides/statistical_insights.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "import logging.config\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import woodwork as ww\n",
    "\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "import evalml\n",
    "from evalml.data_checks import DataCheck, DataChecks, HighlyNullDataCheck, NoVarianceDataCheck, ClassImbalanceDataCheck, TargetLeakageDataCheck, InvalidTargetDataCheck, IDColumnsDataCheck, MulticollinearityDataCheck, OutliersDataCheck\n",
    "from evalml.model_understanding import confusion_matrix, get_prediction_vs_actual_data, explain_predictions, explain_predictions_best_worst\n",
    "from evalml.objectives import get_core_objectives\n",
    "from evalml.objectives.binary_classification_objective import BinaryClassificationObjective\n",
    "from evalml.problem_types import detect_problem_type\n",
    "\n",
    "from evalml.automl import AutoMLSearch\n",
    "\n",
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ww.config.set_option('numeric_categorical_threshold', 2)\n",
    "ww.config.set_option('natural_language_threshold', 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ww.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_kaggle_data():\n",
    "    # token stored in .kaggle/kaggle.json\n",
    "    api = KaggleApi()\n",
    "    api.authenticate()\n",
    "    \n",
    "    # must accept competition rules on kaggle.com\n",
    "    api.competition_download_files('titanic')\n",
    "    \n",
    "    zf = ZipFile('titanic.zip')\n",
    "    zf.extractall('data/')\n",
    "    zf.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_kaggle_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_checks(check, **kwargs):\n",
    "    results = check.validate(**kwargs)\n",
    "    for message in results['warnings']:\n",
    "        print(f\"Warning: {message['message']}\")\n",
    "    for message in results['errors']:\n",
    "        print(f\"Error: {message['message']}\")\n",
    "    \n",
    "def get_relevant_objectives(prob_type):\n",
    "    for objective in get_core_objectives(prob_type):\n",
    "        yield objective.name    \n",
    "    \n",
    "def check_data(x, y):\n",
    "    null_check = HighlyNullDataCheck(pct_null_threshold=0.5)\n",
    "    print_checks(null_check, X=x)\n",
    "    \n",
    "    nv_check = NoVarianceDataCheck()\n",
    "    print_checks(nv_check, X=x, y=y)\n",
    "    \n",
    "    ci_check = ClassImbalanceDataCheck(threshold=0.1)\n",
    "    print_checks(ci_check, X=x, y=y)\n",
    "    \n",
    "    tl_check = TargetLeakageDataCheck(pct_corr_threshold=0.7)\n",
    "    print_checks(tl_check, X=x, y=y)\n",
    "    \n",
    "    prob_type = str(detect_problem_type(y))\n",
    "    for obj in get_relevant_objectives(prob_type):\n",
    "        inv_check = InvalidTargetDataCheck(prob_type, obj)\n",
    "        print_checks(inv_check, X=x, y=y)\n",
    "    \n",
    "    id_check = IDColumnsDataCheck(id_threshold=0.9)\n",
    "    print_checks(id_check, X=x, y=y)\n",
    "    \n",
    "    mc_check = MulticollinearityDataCheck(threshold=0.8)\n",
    "    print_checks(mc_check, X=x, y=y)\n",
    "    \n",
    "    out_check = OutliersDataCheck()\n",
    "    print_checks(out_check, X=x, y=y)\n",
    "    \n",
    "    \n",
    "def process_kaggle_data(split_name, index = 'PassengerId', y = 'Survived'):\n",
    "    data = pd.read_csv(f'data/{split_name}.csv')\n",
    "    \n",
    "    x_df = data.drop([y, index, 'Name', 'Ticket'], axis = 1)\n",
    "    y_df = data[y]\n",
    "    print(detect_problem_type(y_df))\n",
    "    \n",
    "    check_data(x_df, y_df)\n",
    "    \n",
    "    return x_df, y_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = process_kaggle_data('train')\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalml.preprocessing.target_distribution(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_checks = DataChecks(data_checks=[HighlyNullDataCheck, NoVarianceDataCheck, \n",
    "                                      ClassImbalanceDataCheck, TargetLeakageDataCheck, \n",
    "                                      InvalidTargetDataCheck, IDColumnsDataCheck,\n",
    "                                      MulticollinearityDataCheck, OutliersDataCheck],\n",
    "                        data_check_params={'InvalidTargetDataCheck':{'problem_type':'binary',\n",
    "                                                                     'objective':'Log Loss Binary'}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = evalml.preprocessing.split_data(X, y, \n",
    "                                                                   problem_type = 'binary',\n",
    "                                                                   test_size=0.2, random_seed=44133)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SLA(BinaryClassificationObjective):\n",
    "    name = \"Sensitivity at Low Alert Rates\"\n",
    "    greater_is_better = True\n",
    "    score_needs_proba = True\n",
    "    perfect_score = 1.0\n",
    "    is_bounded_like_percentage = True\n",
    "    \n",
    "    def __init__(self, alert_rate=0.01):\n",
    "        \"\"\"Create instance of SLA\n",
    "        \n",
    "        Arguments:\n",
    "            alert_rate (float): percentage of top scores to use in calculating sensitivity\n",
    "        \n",
    "        \"\"\"\n",
    "        self.alert_rate = alert_rate\n",
    "        \n",
    "    def objective_function(self, y_true,  ypred_proba, X=None):\n",
    "        \"\"\"Calculate sensitivity for the top alert_rate % observations\n",
    "        \n",
    "        Arguments:\n",
    "            y_true (pd.Series): true labels\n",
    "            ypred_proba (pd.Series): predicted probabilities\n",
    "        \n",
    "        Returns:\n",
    "            float: sensitivity for the observations with the top predicted probabilities\n",
    "        \"\"\"\n",
    "        \n",
    "        if not isinstance(ypred_proba, pd.Series):\n",
    "            ypred_proba = pd.Series(ypred_proba)\n",
    "            \n",
    "        if not isinstance(y_true, pd.Series):\n",
    "            y_true = pd.Series(y_true)\n",
    "            \n",
    "        prob_thresh = np.quantile(ypred_proba, 1-self.alert_rate)\n",
    "        logging.info(f\"Calculating sensitivity at threshold {prob_thresh}\")\n",
    "        high_risk = ypred_proba.astype(float) >= prob_thresh\n",
    "        \n",
    "        tp = y_true & high_risk\n",
    "        fn = y_true & (~high_risk)\n",
    "        # TODO: tp.sum() + fn.sum() > 0\n",
    "        # TODO: prob_thresh = 0 / all ypred_proba are 0\n",
    "        sensitivity = tp.sum()/(tp.sum()+fn.sum())\n",
    "        \n",
    "        return sensitivity\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sla_objective = SLA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalml.pipelines.components.utils.allowed_model_families('binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl = AutoMLSearch(X_train, y_train, \n",
    "                      problem_type = 'binary',\n",
    "                      objective=sla_objective,\n",
    "                      max_time = 1000,\n",
    "                      allowed_model_families=['random_forest','decision_tree','catboost',\n",
    "                                              'linear_model','extra_trees'],\n",
    "                      ensembling=True\n",
    "                     )\n",
    "\n",
    "logging.config.dictConfig({\n",
    "    'version': 1,\n",
    "    'disable_existing_loggers': True,\n",
    "})\n",
    "\n",
    "automl.search(data_checks=data_checks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl.full_rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_id = automl.full_rankings[automl.full_rankings.score != 1].head(1)['id'].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.config.dictConfig({\n",
    "    'version': 1,\n",
    "    'disable_existing_loggers': False,\n",
    "})\n",
    "automl.describe_pipeline(automl.full_rankings['id'][selected_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline = automl.best_pipeline\n",
    "pipeline = automl.get_pipeline(selected_id)\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.score(X_test, y_test, objectives = [\"auc\",sla_objective])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = pipeline.predict_proba(X_test)[True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explain_predictions(pipeline, X_train, y_train, [0], include_shap_values = True, output_format = \"dataframe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explain_predictions_best_worst(pipeline, X_train, y_train, num_to_explain = 2, \n",
    "                               top_k_features = 3, include_shap_values = True,\n",
    "                              output_format=\"dataframe\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sandbox-dt0F7N9f",
   "language": "python",
   "name": "sandbox-dt0f7n9f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
